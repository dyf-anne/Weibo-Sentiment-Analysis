{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba.posseg as pseg\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# 加载常用停用词\n",
    "stopwords1 = [line.rstrip() for line in open('./中文停用词库.txt', 'r', encoding='utf-8')]\n",
    "# stopwords2 = [line.rstrip() for line in open('./哈工大停用词表.txt', 'r', encoding='utf-8')]\n",
    "# stopwords3 = [line.rstrip() for line in open('./四川大学机器智能实验室停用词库.txt', 'r', encoding='utf-8')]\n",
    "# stopwords = stopwords1 + stopwords2 + stopwords3\n",
    "stopwords = stopwords1\n",
    "\n",
    "\n",
    "def proc_text(raw_line):\n",
    "    \"\"\"\n",
    "        处理每行的文本数据\n",
    "        返回分词结果\n",
    "    \"\"\"\n",
    "    # 1. 使用正则表达式去除非中文字符\n",
    "    filter_pattern = re.compile('[^\\u4E00-\\u9FD5]+')\n",
    "    chinese_only = filter_pattern.sub('', raw_line)\n",
    "\n",
    "    # 2. 结巴分词+词性标注\n",
    "    words_lst = pseg.cut(chinese_only)\n",
    "\n",
    "    # 3. 去除停用词\n",
    "    meaninful_words = []\n",
    "    for word, flag in words_lst:\n",
    "        # if (word not in stopwords) and (flag == 'v'):\n",
    "            # 也可根据词性去除非动词等\n",
    "        if word not in stopwords:\n",
    "            meaninful_words.append(word)\n",
    "\n",
    "    return ' '.join(meaninful_words)\n",
    "\n",
    "\n",
    "def split_train_test(text_df, size=0.8):\n",
    "    \"\"\"\n",
    "        分割训练集和测试集\n",
    "    \"\"\"\n",
    "    # 为保证每个类中的数据能在训练集中和测试集中的比例相同，所以需要依次对每个类进行处理\n",
    "    train_text_df = pd.DataFrame()\n",
    "    test_text_df = pd.DataFrame()\n",
    "\n",
    "    labels = [0, 1, 2, 3]\n",
    "    for label in labels:\n",
    "        # 找出label的记录\n",
    "        text_df_w_label = text_df[text_df['label'] == label]\n",
    "        # 重新设置索引，保证每个类的记录是从0开始索引，方便之后的拆分\n",
    "        text_df_w_label = text_df_w_label.reset_index()\n",
    "\n",
    "        # 默认按80%训练集，20%测试集分割\n",
    "        # 这里为了简化操作，取前80%放到训练集中，后20%放到测试集中\n",
    "        # 当然也可以随机拆分80%，20%（尝试实现下DataFrame中的随机拆分）\n",
    "\n",
    "        # 该类数据的行数\n",
    "        n_lines = text_df_w_label.shape[0]\n",
    "        split_line_no = math.floor(n_lines * size)\n",
    "        text_df_w_label_train = text_df_w_label.iloc[:split_line_no, :]\n",
    "        text_df_w_label_test = text_df_w_label.iloc[split_line_no:, :]\n",
    "\n",
    "        # 放入整体训练集，测试集中\n",
    "        train_text_df = train_text_df.append(text_df_w_label_train)\n",
    "        test_text_df = test_text_df.append(text_df_w_label_test)\n",
    "\n",
    "    train_text_df = train_text_df.reset_index()\n",
    "    test_text_df = test_text_df.reset_index()\n",
    "    return train_text_df, test_text_df\n",
    "\n",
    "\n",
    "def get_word_list_from_data(text_df):\n",
    "    \"\"\"\n",
    "        将数据集中的单词放入到一个列表中\n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "    for _, r_data in text_df.iterrows():\n",
    "        word_list += r_data['text'].split(' ')\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def extract_feat_from_data(text_df, text_collection, common_words_freqs):\n",
    "    \"\"\"\n",
    "        特征提取\n",
    "    \"\"\"\n",
    "    # 这里只选择TF-IDF特征作为例子\n",
    "    # 可考虑使用词频或其他文本特征作为额外的特征\n",
    "\n",
    "    n_sample = text_df.shape[0]\n",
    "    n_feat = len(common_words_freqs)\n",
    "    common_words = [word for word, _ in common_words_freqs]\n",
    "\n",
    "    # 初始化\n",
    "    X = np.zeros([n_sample, n_feat])\n",
    "    y = np.zeros(n_sample)\n",
    "\n",
    "    print('提取特征...')\n",
    "    for i, r_data in text_df.iterrows():\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print('已完成{}个样本的特征提取'.format(i + 1))\n",
    "\n",
    "        text = r_data['text']\n",
    "\n",
    "        feat_vec = []\n",
    "        for word in common_words:\n",
    "            if word in text:\n",
    "                # 如果在高频词中，计算TF-IDF值\n",
    "                tf_idf_val = text_collection.tf_idf(word, text)\n",
    "            else:\n",
    "                tf_idf_val = 0\n",
    "\n",
    "            feat_vec.append(tf_idf_val)\n",
    "\n",
    "        # 赋值\n",
    "        X[i, :] = np.array(feat_vec)\n",
    "        y[i] = int(r_data['label'])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def cal_acc(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "        计算准确率\n",
    "    \"\"\"\n",
    "    n_total = len(true_labels)\n",
    "    correct_list = [true_labels[i] == pred_labels[i] for i in range(n_total)]\n",
    "\n",
    "    acc = sum(correct_list) / n_total\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理清洗文本数据中... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/tm/_s0pdrcx74l71x08t1d9xrmm0000gn/T/jieba.cache\n",
      "Loading model cost 0.709 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成，并保存结果。\n",
      "加载处理好的文本数据\n",
      "训练集中各类的数据个数： label\n",
      "0    127294\n",
      "1     33021\n",
      "2     35242\n",
      "3     16823\n",
      "dtype: int64\n",
      "测试集中各类的数据个数： label\n",
      "0    31824\n",
      "1     8256\n",
      "2     8811\n",
      "3     4206\n",
      "dtype: int64\n",
      "统计词频...\n",
      "出现最多的200个词是：\n",
      "说: 28132次\n",
      "一个: 25375次\n",
      "会: 21120次\n",
      "想: 16914次\n",
      "太: 16252次\n",
      "回复: 16166次\n",
      "没有: 14125次\n",
      "今天: 13077次\n",
      "没: 12930次\n",
      "爱: 12899次\n",
      "中: 12711次\n",
      "做: 12507次\n",
      "喜欢: 11602次\n",
      "吃: 11530次\n",
      "中国: 11370次\n",
      "月: 10809次\n",
      "知道: 10739次\n",
      "现在: 10095次\n",
      "真的: 9385次\n",
      "真: 9163次\n",
      "看到: 9151次\n",
      "日: 8383次\n",
      "哈哈哈: 7738次\n",
      "里: 7706次\n",
      "请: 7693次\n",
      "不要: 7690次\n",
      "可爱: 7376次\n",
      "更: 7274次\n",
      "朋友: 7194次\n",
      "时: 6957次\n",
      "死: 6860次\n",
      "年: 6780次\n",
      "点: 6732次\n",
      "已经: 6522次\n",
      "一下: 6505次\n",
      "笑: 6319次\n",
      "觉得: 6210次\n",
      "希望: 5943次\n",
      "一起: 5943次\n",
      "一定: 5924次\n",
      "不能: 5912次\n",
      "男人: 5834次\n",
      "粉丝: 5783次\n",
      "转: 5642次\n",
      "关注: 5618次\n",
      "不会: 5545次\n",
      "分享: 5496次\n",
      "买: 5435次\n",
      "女人: 5430次\n",
      "快: 5378次\n",
      "微博: 5264次\n",
      "走: 5155次\n",
      "时间: 5130次\n",
      "图片: 5008次\n",
      "支持: 4975次\n",
      "生活: 4949次\n",
      "图: 4917次\n",
      "应该: 4916次\n",
      "孩子: 4708次\n",
      "起来: 4690次\n",
      "发现: 4679次\n",
      "世界: 4660次\n",
      "找: 4657次\n",
      "真是: 4655次\n",
      "谢谢: 4621次\n",
      "其实: 4544次\n",
      "一直: 4528次\n",
      "新: 4505次\n",
      "看看: 4482次\n",
      "感觉: 4448次\n",
      "北京: 4443次\n",
      "微: 4409次\n",
      "明天: 4358次\n",
      "幸福: 4322次\n",
      "活动: 4271次\n",
      "照片: 4210次\n",
      "最后: 4201次\n",
      "博: 4167次\n",
      "很多: 4151次\n",
      "快乐: 4050次\n",
      "前: 4005次\n",
      "岁: 3977次\n",
      "终于: 3976次\n",
      "老师: 3973次\n",
      "老: 3967次\n",
      "事: 3966次\n",
      "张: 3947次\n",
      "围脖: 3940次\n",
      "可能: 3932次\n",
      "听: 3878次\n",
      "出: 3856次\n",
      "原来: 3828次\n",
      "天: 3821次\n",
      "问题: 3729次\n",
      "东西: 3711次\n",
      "日本: 3662次\n",
      "工作: 3644次\n",
      "需要: 3643次\n",
      "不错: 3619次\n",
      "送: 3586次\n",
      "一次: 3560次\n",
      "加油: 3543次\n",
      "美: 3514次\n",
      "先: 3476次\n",
      "带: 3459次\n",
      "话: 3449次\n",
      "一天: 3444次\n",
      "一种: 3441次\n",
      "拍: 3409次\n",
      "开心: 3405次\n",
      "见: 3376次\n",
      "男: 3366次\n",
      "手机: 3355次\n",
      "玩: 3355次\n",
      "机会: 3343次\n",
      "钱: 3336次\n",
      "感谢: 3293次\n",
      "成为: 3277次\n",
      "女: 3268次\n",
      "以后: 3255次\n",
      "有人: 3229次\n",
      "问: 3193次\n",
      "同学: 3187次\n",
      "上海: 3185次\n",
      "写: 3174次\n",
      "啊啊啊: 3120次\n",
      "有点: 3101次\n",
      "唔: 3082次\n",
      "这种: 3068次\n",
      "元: 3062次\n",
      "第名: 3021次\n",
      "高: 3011次\n",
      "每天: 2990次\n",
      "美国: 2951次\n",
      "期待: 2936次\n",
      "非常: 2930次\n",
      "两个: 2920次\n",
      "晚上: 2912次\n",
      "完: 2903次\n",
      "继续: 2855次\n",
      "发: 2826次\n",
      "睡: 2821次\n",
      "穿: 2813次\n",
      "哈哈哈哈: 2809次\n",
      "人生: 2795次\n",
      "昨天: 2785次\n",
      "星座: 2728次\n",
      "系: 2707次\n",
      "是不是: 2706次\n",
      "永远: 2704次\n",
      "地方: 2695次\n",
      "滴: 2691次\n",
      "公司: 2684次\n",
      "一张: 2646次\n",
      "评论: 2613次\n",
      "最近: 2603次\n",
      "居然: 2601次\n",
      "小时: 2588次\n",
      "刚: 2580次\n",
      "心: 2561次\n",
      "开: 2561次\n",
      "长: 2555次\n",
      "事情: 2554次\n",
      "视频: 2547次\n",
      "一点: 2534次\n",
      "回家: 2524次\n",
      "电影: 2512次\n",
      "妈妈: 2506次\n",
      "好好: 2458次\n",
      "新浪: 2453次\n",
      "国家: 2430次\n",
      "回来: 2429次\n",
      "哭: 2428次\n",
      "搞: 2421次\n",
      "感动: 2420次\n",
      "家: 2408次\n",
      "今晚: 2404次\n",
      "告诉: 2397次\n",
      "必须: 2395次\n",
      "美女: 2393次\n",
      "之后: 2390次\n",
      "爱情: 2368次\n",
      "突然: 2354次\n",
      "童鞋: 2345次\n",
      "成: 2334次\n",
      "现场: 2332次\n",
      "牛: 2322次\n",
      "广州: 2320次\n",
      "心情: 2319次\n",
      "获得: 2311次\n",
      "号: 2310次\n",
      "记得: 2308次\n",
      "挺: 2307次\n",
      "下午: 2282次\n",
      "猫: 2265次\n",
      "懂: 2262次\n",
      "特别: 2257次\n",
      "社会: 2249次\n",
      "神马: 2235次\n",
      "准备: 2195次\n",
      "\n",
      "训练样本提取特征... 提取特征...\n",
      "已完成5000个样本的特征提取\n",
      "已完成10000个样本的特征提取\n",
      "已完成15000个样本的特征提取\n",
      "已完成20000个样本的特征提取\n",
      "已完成25000个样本的特征提取\n",
      "已完成30000个样本的特征提取\n",
      "已完成35000个样本的特征提取\n",
      "已完成40000个样本的特征提取\n",
      "已完成45000个样本的特征提取\n",
      "已完成50000个样本的特征提取\n",
      "已完成55000个样本的特征提取\n",
      "已完成60000个样本的特征提取\n",
      "已完成65000个样本的特征提取\n",
      "已完成70000个样本的特征提取\n",
      "已完成75000个样本的特征提取\n",
      "已完成80000个样本的特征提取\n",
      "已完成85000个样本的特征提取\n",
      "已完成90000个样本的特征提取\n",
      "已完成95000个样本的特征提取\n",
      "已完成100000个样本的特征提取\n",
      "已完成105000个样本的特征提取\n",
      "已完成110000个样本的特征提取\n",
      "已完成115000个样本的特征提取\n",
      "已完成120000个样本的特征提取\n",
      "已完成125000个样本的特征提取\n",
      "已完成130000个样本的特征提取\n",
      "已完成135000个样本的特征提取\n",
      "已完成140000个样本的特征提取\n",
      "已完成145000个样本的特征提取\n",
      "已完成150000个样本的特征提取\n",
      "已完成155000个样本的特征提取\n",
      "已完成160000个样本的特征提取\n",
      "已完成165000个样本的特征提取\n",
      "已完成170000个样本的特征提取\n",
      "已完成175000个样本的特征提取\n",
      "已完成180000个样本的特征提取\n",
      "已完成185000个样本的特征提取\n",
      "已完成190000个样本的特征提取\n",
      "已完成195000个样本的特征提取\n",
      "已完成200000个样本的特征提取\n",
      "已完成205000个样本的特征提取\n",
      "已完成210000个样本的特征提取\n",
      "完成\n",
      "\n",
      "测试样本提取特征... 提取特征...\n",
      "已完成5000个样本的特征提取\n",
      "已完成10000个样本的特征提取\n",
      "已完成15000个样本的特征提取\n",
      "已完成20000个样本的特征提取\n",
      "已完成25000个样本的特征提取\n",
      "已完成30000个样本的特征提取\n",
      "已完成35000个样本的特征提取\n",
      "已完成40000个样本的特征提取\n",
      "已完成45000个样本的特征提取\n",
      "已完成50000个样本的特征提取\n",
      "完成\n",
      "训练模型... 完成\n",
      "\n",
      "测试模型... 完成\n",
      "准确率： 0.297869936154585\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from tools import proc_text, split_train_test, get_word_list_from_data, \\\n",
    "    extract_feat_from_data, cal_acc\n",
    "from nltk.text import TextCollection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "dataset_path = './dataset'\n",
    "text_filenames = ['0_simplifyweibo.txt', '1_simplifyweibo.txt',\n",
    "                  '2_simplifyweibo.txt', '3_simplifyweibo.txt']\n",
    "\n",
    "# 原始数据的csv文件\n",
    "output_text_filename = 'raw_weibo_text.csv'\n",
    "\n",
    "# 清洗好的文本数据文件\n",
    "output_cln_text_filename = 'clean_weibo_text.csv'\n",
    "\n",
    "# 处理和清洗文本数据的时间较长，通过设置is_first_run进行配置\n",
    "# 如果是第一次运行需要对原始文本数据进行处理和清洗，需要设为True\n",
    "# 如果之前已经处理了文本数据，并已经保存了清洗好的文本数据，设为False即可\n",
    "is_first_run = True\n",
    "\n",
    "\n",
    "def read_and_save_to_csv():\n",
    "    \"\"\"\n",
    "        读取原始文本数据，将标签和文本数据保存成csv\n",
    "    \"\"\"\n",
    "\n",
    "    text_w_label_df_lst = []\n",
    "    for text_filename in text_filenames:\n",
    "        text_file = os.path.join(dataset_path, text_filename)\n",
    "\n",
    "        # 获取标签，即0, 1, 2, 3\n",
    "        label = int(text_filename[0])\n",
    "\n",
    "        # 读取文本文件\n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        labels = [label] * len(lines)\n",
    "\n",
    "        text_series = pd.Series(lines)\n",
    "        label_series = pd.Series(labels)\n",
    "\n",
    "        # 构造dataframe\n",
    "        text_w_label_df = pd.concat([label_series, text_series], axis=1)\n",
    "        text_w_label_df_lst.append(text_w_label_df)\n",
    "\n",
    "    result_df = pd.concat(text_w_label_df_lst, axis=0)\n",
    "\n",
    "    # 保存成csv文件\n",
    "    result_df.columns = ['label', 'text']\n",
    "    result_df.to_csv(os.path.join(dataset_path, output_text_filename),\n",
    "                     index=None, encoding='utf-8')\n",
    "\n",
    "\n",
    "def run_main():\n",
    "    \"\"\"\n",
    "        主函数\n",
    "    \"\"\"\n",
    "    # 1. 数据读取，处理，清洗，准备\n",
    "    if is_first_run:\n",
    "        print('处理清洗文本数据中...', end=' ')\n",
    "        # 如果是第一次运行需要对原始文本数据进行处理和清洗\n",
    "\n",
    "        # 读取原始文本数据，将标签和文本数据保存成csv\n",
    "        read_and_save_to_csv()\n",
    "\n",
    "        # 读取处理好的csv文件，构造数据集\n",
    "        text_df = pd.read_csv(os.path.join(dataset_path, output_text_filename),\n",
    "                              encoding='utf-8')\n",
    "\n",
    "        # 处理文本数据\n",
    "        text_df['text'] = text_df['text'].apply(proc_text)\n",
    "\n",
    "        # 过滤空字符串\n",
    "        text_df = text_df[text_df['text'] != '']\n",
    "\n",
    "        # 保存处理好的文本数据\n",
    "        text_df.to_csv(os.path.join(dataset_path, output_cln_text_filename),\n",
    "                       index=None, encoding='utf-8')\n",
    "        print('完成，并保存结果。')\n",
    "\n",
    "    # 2. 分割训练集、测试集\n",
    "    print('加载处理好的文本数据')\n",
    "    clean_text_df = pd.read_csv(os.path.join(dataset_path, output_cln_text_filename),\n",
    "                                encoding='utf-8')\n",
    "    # 分割训练集和测试集\n",
    "    train_text_df, test_text_df = split_train_test(clean_text_df)\n",
    "    # 查看训练集测试集基本信息\n",
    "    print('训练集中各类的数据个数：', train_text_df.groupby('label').size())\n",
    "    print('测试集中各类的数据个数：', test_text_df.groupby('label').size())\n",
    "\n",
    "    # 3. 特征提取\n",
    "    # 计算词频\n",
    "    n_common_words = 200\n",
    "\n",
    "    # 将训练集中的单词拿出来统计词频\n",
    "    print('统计词频...')\n",
    "    all_words_in_train = get_word_list_from_data(train_text_df)\n",
    "    fdisk = nltk.FreqDist(all_words_in_train)\n",
    "    common_words_freqs = fdisk.most_common(n_common_words)\n",
    "    print('出现最多的{}个词是：'.format(n_common_words))\n",
    "    for word, count in common_words_freqs:\n",
    "        print('{}: {}次'.format(word, count))\n",
    "    print()\n",
    "\n",
    "    # 在训练集上提取特征\n",
    "    text_collection = TextCollection(train_text_df['text'].values.tolist())\n",
    "    print('训练样本提取特征...', end=' ')\n",
    "    train_X, train_y = extract_feat_from_data(train_text_df, text_collection, common_words_freqs)\n",
    "    print('完成')\n",
    "    print()\n",
    "\n",
    "    print('测试样本提取特征...', end=' ')\n",
    "    test_X, test_y = extract_feat_from_data(test_text_df, text_collection, common_words_freqs)\n",
    "    print('完成')\n",
    "\n",
    "    # 4. 训练模型Naive Bayes\n",
    "    print('训练模型...', end=' ')\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(train_X, train_y)\n",
    "    print('完成')\n",
    "    print()\n",
    "\n",
    "    # 5. 预测\n",
    "    print('测试模型...', end=' ')\n",
    "    test_pred = gnb.predict(test_X)\n",
    "    print('完成')\n",
    "\n",
    "    # 输出准确率\n",
    "    print('准确率：', cal_acc(test_y, test_pred))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
